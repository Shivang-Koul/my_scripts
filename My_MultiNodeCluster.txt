#!/bin/bash
# Multi-Node Hadoop Cluster Setup Script
# This script assumes 6 machines: nn (NameNode), snn (Secondary NameNode), rm (Resourse Manager), dn1, dn2, dn3 (DataNodes)

# Step 1: Configure Internal DNS

Configuring sudo nano /etc/hosts on all machines (pvt IP)
  
172.31.13.139 nn
172.31.7.187 snn
172.31.12.83 rm
172.31.11.150 dn1
172.31.11.22 dn2
172.31.13.88 dn3

ssh (Private IP For All 5 Machines)
arp (All 6 Machines are connected)
# Step 2: Set Up Passwordless SSH
  scp -i Keyname.pem Keyname.pem ubuntu@172.31.0.208:/home/ubuntu/.ssh
  cd ~/.ssh
  chmod 400 Keyname.pem
ssh -i rdMUM.pem (nn,snn,jt,dn1,dn2,dn3)

nano .profile

eval "$(ssh-agent -s)" ssh-add /home/ubuntu/.ssh/key.pem

source .profile

# Step 3: Enable Two-Way Communication
scp ~/.profile snn:~/ (sending .profile to all machines )
scp ~/.ssh/key.pem snn:/home/ubuntu/.ssh/ (sending key to all machines)
ssh (nn,snn,jt,1dn,2dn,3dn)
sudo nano /etc/hosts

172.31.13.139 nn
172.31.7.187 snn
172.31.12.83 rm
172.31.11.150 dn1
172.31.11.22 dn2
172.31.13.88 dn3

sudo hostname (nn,snn,rm,1dn,2dn,3dn)
sudo nano /etc/ssh/ssh_config  (For Not Asking YES/NO)
#StrictHostKeyChecking ask   -----CHANGE TO---------   StrictHostKeyChecking no (do it all machines)
                  
# Step 4: Install and Configure DSH (for handling all machines at ones dsh)
  sudo apt update
  sudo apt install -y dsh
  sudo nano /etc/dsh/machines.list  (Put All Machines Name)
nn
snn
rm
dn1
dn2
dn3

dsh -a sudo apt update 
(getting error possibly:By default, dsh may be trying to use rsh (which is outdated and often disabled). You need to configure dsh to use ssh instead. SOLUTION= echo "remoteshell ssh" | sudo tee -a /etc/dsh/dsh.conf)


# Step 5: Install Java and Hadoop
  dsh -a sudo apt install -y openjdk-8-jre
  dsh -a sudo apt install -y openjdk-8-jdk-headless
  dsh -a wget https://archive.apache.org/dist/hadoop/common/hadoop-1.2.1/hadoop-1.2.1.tar.gz
  dsh -a tar -zxf hadoop-1.2.1.tar.gz
  dsh -a sudo mv hadoop-1.2.1 /usr/local/hadoop


# Step 6: Configure Hadoop

  # Update ~/.bashrc
  nano ~/.bashrc
export HADOOP_PREFIX=/usr/local/hadoop
export PATH=\$PATH:\$HADOOP_PREFIX/bin
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
export PATH=\$PATH:\$JAVA_HOME

  source ~/.bashrc
scp .bashrc jt:~/ 
  # Configure Hadoop files (on nn)
cd /usr/local/hadoop/conf
nano hadoop-env.sh
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
export HADOOP_OPTS=-Djava.net.preferIPV4Stack=true


  nano core-site.xml
<property>
<name>fs.default.name</name>
<value>hdfs://nn:9000</value>                        
</property>

<property>
<name>hadoop.tmp.dir</name>
<value>/usr/local/hadoop/tmp</value>
</property> 


  nano mapred-site.xml

  <property>
    <name>mapred.job.tracker</name>
    <value>hdfs://rm:9001</value>
  </property>
  
  nano /usr/local/hadoop/conf/hdfs-site.xml

<property>
<name>dfs.replication</name>
<value>3</value>
</property>
<property> 
<name>dfs.permissions</name> 
<value>false</value> 
</property>
*********************

nano slaves
dn1
dn2
dn3


nano masters
snn
# Step 7: Distribute Configuration Files
/usr/local/hadoop/conf$ scp hdfs-site.xml hadoop-env.sh core-site.xml mapred-site.xml slaves masters snn:/usr/local/hadoop/conf/

# Step 8: Prepare and Format HDFS (make sure you are in nn)

ssh nn 

dsh -a mkdir /usr/local/hadoop/tmp
dsh -a exec bash
hadoop namenode -format

# Step 9: Start Hadoop Services

  ssh jt start-mapred.sh (make sure you are on jt)
  ssh nn start-dfs.sh (make sure you are on nn)


# Step 10: Verify Services

  dsh -a jps


# Main Execution
configure_dns
setup_passwordless_ssh
enable_two_way_communication
install_dsh
install_java_and_hadoop
configure_hadoop
send_configuration_files
prepare_hdfs
start_hadoop_services
verify_hadoop_services

